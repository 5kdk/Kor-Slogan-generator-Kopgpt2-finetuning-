{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from transformers import PreTrainedTokenizerFast\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "\r\n",
    "MODEL_NAME = \"skt/kogpt2-base-v2\"\r\n",
    "MODEL_PATH = \"./models\"\r\n",
    "SEQ_LEN = 50\r\n",
    "TOKENS_DICT = {\r\n",
    "    'eos_token':'</s>',\r\n",
    "    'pad_token':'<pad>',\r\n",
    "    'additional_special_tokens':['<context>', '<slogan>'],\r\n",
    "}\r\n",
    "\r\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\r\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\r\n",
    "tokenizer.add_special_tokens(TOKENS_DICT)\r\n",
    "model.resize_token_embeddings(len(tokenizer))\r\n",
    "\r\n",
    "print(tokenizer.special_tokens_map)\r\n",
    "print(model.resize_token_embeddings(len(tokenizer)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eos_token': '</s>', 'pad_token': '<pad>', 'additional_special_tokens': \"['<context>', '<slogan>']\"}\n",
      "Embedding(51202, 768)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# HuggingFace에서 top k와 top p로 함수 샘플링\r\n",
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "from tqdm import trange\r\n",
    "\r\n",
    "\r\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\r\n",
    "\r\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\r\n",
    "    if top_k > 0:\r\n",
    "        # top-k의 마지막 토큰보다 확률이 낮은 모든 토큰을 제거\r\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\r\n",
    "        logits[indices_to_remove] = filter_value\r\n",
    "\r\n",
    "    if top_p > 0.0:\r\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\r\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\r\n",
    "\r\n",
    "        # 임계값 이상의 누적 확률을 가진 토큰 제거\r\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\r\n",
    "        \r\n",
    "        # 첫 번째 토큰도 임계값보다 높게 유지하려면 인덱스를 오른쪽으로 이동\r\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\r\n",
    "        sorted_indices_to_remove[..., 0] = 0\r\n",
    "\r\n",
    "        # 정렬된 텐서를 원래 인덱싱에 분산\r\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\r\n",
    "        logits[indices_to_remove] = filter_value\r\n",
    "    return logits\r\n",
    "\r\n",
    "\r\n",
    "# HuggingFace에서 컨텍스트/슬로건 분리 작업에 맞게 조정됨\r\n",
    "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\r\n",
    "                    device='cpu'):\r\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\r\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\r\n",
    "    generated = context\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for _ in trange(length):\r\n",
    "\r\n",
    "            inputs = {'input_ids': generated}\r\n",
    "            if segments_tokens != None:\r\n",
    "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\r\n",
    "\r\n",
    "\r\n",
    "            outputs = model(**inputs)\r\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\r\n",
    "\r\n",
    "            # CTRL의 반복 페널티(https://arxiv.org/abs/1909.05858)\r\n",
    "            for i in range(num_samples):\r\n",
    "                for _ in set(generated[i].tolist()):\r\n",
    "                    next_token_logits[i, _] /= repetition_penalty\r\n",
    "                \r\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\r\n",
    "            if temperature == 0: # greedy sampling\r\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\r\n",
    "            else:\r\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\r\n",
    "            generated = torch.cat((generated, next_token), dim=1)\r\n",
    "    return generated\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "context = \"드립 커피 전문점\"\r\n",
    "\r\n",
    "context_tkn = tokenizer.additional_special_tokens_ids[0]\r\n",
    "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\r\n",
    "\r\n",
    "input_ids = [context_tkn] + tokenizer.encode(context)\r\n",
    "\r\n",
    "segments = [slogan_tkn] * SEQ_LEN\r\n",
    "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\r\n",
    "\r\n",
    "input_ids += [slogan_tkn]\r\n",
    "\r\n",
    "\r\n",
    "epochs = [2, 5, 10]\r\n",
    "for epoch in epochs:\r\n",
    "  model.load_state_dict(torch.load(MODEL_PATH+ '/' + f'processed_slogan_{epoch}epoch_model.pth'))\r\n",
    "  model.eval()\r\n",
    "\r\n",
    "  # 최개길이 20의 20개의 슬로건 샘플\r\n",
    "  # 확률분포를 조금 뾰족하게 하여 확률값이 높은 토큰이 살짝 더 잘나오도록 (temperature=0.9)\r\n",
    "  # top_k 샘플링을 적용하여 확률값이 낮은 토큰들은 후보 단어에서 배제 (top_k=5)\r\n",
    "  generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, temperature=0.9, top_k=5, num_samples=20)\r\n",
    "\r\n",
    "  print('\\n\\n--- Generated Slogans ---\\n')\r\n",
    "\r\n",
    "  for g in generated:\r\n",
    "    slogan = tokenizer.decode(g.squeeze().tolist())\r\n",
    "    slogan = slogan.split('</s>')[0].split('<slogan>')[1]\r\n",
    "    print(slogan)   "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 당신의 아삭하게 \n",
      " 당신의 아빠의 \n",
      " 우리아이 한국을 더, 당신을 위한 내 순하게  \n",
      " 대한민국, 내일을 위해 \n",
      " 당신의 모든 모든 일상을 위한 \n",
      " 우리, 당신이 행복을 위해 \n",
      " 모든 일상을 위해 \n",
      " 대한민국 한국을 바꾸다, 당신을 바꾸다 \n",
      " 우리 가족의 당신이 더 \n",
      " 대한민국, 내일의 당신은 한국을 위한 \n",
      " 당신을 위한 당신이 더 더 \n",
      " 당신은 행복을 위한 당상을 바꾸다 \n",
      " 당신은 행복한 \n",
      " 모든 일상을 바꾸다 \n",
      " 당신의 모든 모든 일상을 바꾸다 \n",
      " 우리만의 당신이 내 순상을 위한 \n",
      " 모든 일상을 바꾸다 \n",
      " 당신의 모든 일상을 위한 힘다 \n",
      " 당신이 행복이까 \n",
      " 당신의 한끼, \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " Its them them in worry Fifes \n",
      " Im in your do \n",
      " 당신의 모든 순간, 모든 것 \n",
      " 당신의 새로운 새로운 모든 순간에 \n",
      " 내일의 커피의 기준 \n",
      " ILNEMEESMEME \n",
      " 스타일이 \n",
      " 내 몸에 빛나는 모든 순향존RE \n",
      " 스타일을 위해 \n",
      " 내일도 빛나는 순간 \n",
      " 내일을 바꾸다 \n",
      " 스타일리드 \n",
      " 내 몸에 좋은 카트 \n",
      " ILL IMEE TUST \n",
      " THIS TOREAINE \n",
      " 내 몸이 빛나는 세상 \n",
      " IMEMEMEATEMEIE \n",
      " IN IUTME ISMMEININE \n",
      " 스타일이 커피를 만나다 \n",
      " 스타일은 카카를 바꾸다 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.34it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 커피 예술을 하다 \n",
      " 커피 예술을 만듭니다 \n",
      " 한 번에서 커피이 다했다, 당신을 더 가까이 \n",
      " 커피의 시작이 쌓인다 \n",
      " 커피을 담다, 판타지 않는다 \n",
      " 커피를 담다, 판타스 \n",
      " 당신을 생각하는 세상의 모든 프리미엄가 \n",
      " 프리미엄 커피 \n",
      " 커피를 위한 모든 순간 \n",
      " 커피을 담다 \n",
      " 프리미엄 커피피스 \n",
      " 당신의 선택의 새로운 기준 \n",
      " 프리미엄 커피, 위대한 기준 \n",
      " 당신의 마스터피스피스피스 \n",
      " 당신이 원하는 판타지,라이 \n",
      " 커피가 \n",
      " 당신을 위한 퍼포미엄 혜택 \n",
      " 커피의 새로운 기준 \n",
      " 커피를 위한 모든 것 \n",
      " 당신은 기다리던 커피의 시작 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for epoch in epochs:\r\n",
    "  model.load_state_dict(torch.load(MODEL_PATH+ '/' + f'processed_slogan_{epoch}epoch_model.pth'))\r\n",
    "  model.eval()\r\n",
    "\r\n",
    "  # 최개길이 20의 20개의 슬로건 샘플\r\n",
    "  # 확률분포를 조금 뾰족하게 하여 확률값이 높은 토큰이 살짝 더 잘나오도록 (temperature=0.9)\r\n",
    "  # top_k 샘플링과 top_p 샘플링을 동시에 적용하여 확률값이 낮은 토큰들은 후보 단어에서 배제 (top_k=50, top_p=0.95)\r\n",
    "  generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, temperature=0.9, top_k=20, top_p=0.95, num_samples=20)\r\n",
    "\r\n",
    "  print('\\n\\n--- Generated Slogans ---\\n')\r\n",
    "\r\n",
    "  for g in generated:\r\n",
    "    slogan = tokenizer.decode(g.squeeze().tolist())\r\n",
    "    slogan = slogan.split('</s>')[0].split('<slogan>')[1]\r\n",
    "    print(slogan)   "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 내 차만의 더 행복한 모든 수분의 대한민국 \n",
      " 아끼, \n",
      " 당신의 한 번만, 이 알아 \n",
      " 우리만의 모든 순간, \n",
      " 피부는 우리 가족부터 내 한끼 \n",
      " 이 이상의 \n",
      " 좋은 손만의 일상을 위한 기술 \n",
      " 당신만 \n",
      " 나를 위한 당신이 더 수 없는 아칠든다 \n",
      " 당신의 행복을 깨우다 \n",
      " 대한민국를 위한 금융 \n",
      " 요즘 한분의 한국을 위한 \n",
      " 우리에서도 our \n",
      " 요즘, 더, 일상에 더, 그 \n",
      " 이 장일의 대한민국 \n",
      " 일상을 만듭니다 \n",
      " 한 잔 땐 \n",
      " 대한민국 our Your lour Lree \n",
      " 피침을 더 \n",
      " 모든 손안의 대한민국은 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 스타일은 스타일은 여좀 \n",
      " 내일운 무피크워, 모든 순한 모든국에 보습 \n",
      " 당신의 모든 것을 지배하는 \n",
      " Lont woring Lifortion bot stains your difers \n",
      " 내일은 여정이 필요한 \n",
      " Letter, You \n",
      " 우리아이 한 판도 유 번에 \n",
      " 모든 순간을 위해 \n",
      " 당신의 모든 순간에 \n",
      " 스타일을 찾아보세요 \n",
      " THE TY IY OUY IOTHIF \n",
      " 당신의 모든 것은, SUKGTE NELFE EME \n",
      " 내일의 한 발만의 커피 \n",
      " 한 가지거운 내 몸에 한피 \n",
      " IUT IARUTANL \n",
      " 스타일리거운 요리가 만든 \n",
      " 새로운 클래스를 바꾸다 \n",
      " 스타일로 선택한 모든 순간 \n",
      " 올 겨울도 안 짜릿하게 \n",
      " 스타일이의 새로운 차이 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:15<00:00,  1.32it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 커피와 함께 맛있게 \n",
      " 그 맛의 시작 \n",
      " 그 시절 한 판타, 또 하나의 완벽 \n",
      " 당신이 기다리던 그릭 친구 \n",
      " 건강한 커피라이프를 만나다 \n",
      " 여행도 다 가격해? \n",
      " 한 모금까지 풀코스 \n",
      " 건강한 커피 \n",
      " 또짠 슈퍼저몰을 시작 \n",
      " 커피를 내리다 \n",
      " 당신을 건강하게 준비할 때 \n",
      " 나만의 슈퍼 슈퍼콘 \n",
      " 당신의 새로운 차원의 프리미엄피스 \n",
      " 커피라이라이트로 \n",
      " 나를 위해 \n",
      " 여행의 시작은 아름답다 \n",
      " 오늘도 맛있게 \n",
      " 커피가 바로 지금, 바로 \n",
      " 오늘을 커피를 만듭니다 \n",
      " 커피도 \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)"
  },
  "interpreter": {
   "hash": "293ef13038b1144d4811de228cdfb91e615f2f48e1a0c87d3a386cf88ee0761d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}