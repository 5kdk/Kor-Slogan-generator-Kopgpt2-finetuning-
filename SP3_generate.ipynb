{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import PreTrainedTokenizerFast\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "\r\n",
    "MODEL_NAME = \"skt/kogpt2-base-v2\"\r\n",
    "MODEL_PATH = \"./models\"\r\n",
    "SEQ_LEN = 50\r\n",
    "TOKENS_DICT = {\r\n",
    "    'additional_special_tokens':['<unused0>', '<unused1>'],\r\n",
    "}\r\n",
    "\r\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\r\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\r\n",
    "tokenizer.add_special_tokens(TOKENS_DICT)\r\n",
    "print(tokenizer.special_tokens_map)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'additional_special_tokens': \"['<unused0>', '<unused1>']\"}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# HuggingFace에서 top k와 top p로 함수 샘플링\r\n",
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "from tqdm import trange\r\n",
    "\r\n",
    "\r\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\r\n",
    "\r\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\r\n",
    "    if top_k > 0:\r\n",
    "        # top-k의 마지막 토큰보다 확률이 낮은 모든 토큰을 제거\r\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\r\n",
    "        logits[indices_to_remove] = filter_value\r\n",
    "\r\n",
    "    if top_p > 0.0:\r\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\r\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\r\n",
    "\r\n",
    "        # 임계값 이상의 누적 확률을 가진 토큰 제거\r\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\r\n",
    "        \r\n",
    "        # 첫 번째 토큰도 임계값보다 높게 유지하려면 인덱스를 오른쪽으로 이동\r\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\r\n",
    "        sorted_indices_to_remove[..., 0] = 0\r\n",
    "\r\n",
    "        # 정렬된 텐서를 원래 인덱싱에 분산\r\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\r\n",
    "        logits[indices_to_remove] = filter_value\r\n",
    "    return logits\r\n",
    "\r\n",
    "\r\n",
    "# HuggingFace에서 컨텍스트/슬로건 분리 작업에 맞게 조정됨\r\n",
    "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\r\n",
    "                    device='cpu'):\r\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\r\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\r\n",
    "    generated = context\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for _ in trange(length):\r\n",
    "\r\n",
    "            inputs = {'input_ids': generated}\r\n",
    "            if segments_tokens != None:\r\n",
    "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\r\n",
    "\r\n",
    "\r\n",
    "            outputs = model(**inputs)\r\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\r\n",
    "\r\n",
    "            # CTRL의 반복 페널티(https://arxiv.org/abs/1909.05858)\r\n",
    "            for i in range(num_samples):\r\n",
    "                for _ in set(generated[i].tolist()):\r\n",
    "                    next_token_logits[i, _] /= repetition_penalty\r\n",
    "                \r\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\r\n",
    "            if temperature == 0: # greedy sampling\r\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\r\n",
    "            else:\r\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\r\n",
    "            generated = torch.cat((generated, next_token), dim=1)\r\n",
    "    return generated\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "context = \"드립 커피 전문점\"\r\n",
    "\r\n",
    "context_tkn = tokenizer.additional_special_tokens_ids[0]\r\n",
    "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\r\n",
    "\r\n",
    "input_ids = [context_tkn] + tokenizer.encode(context)\r\n",
    "\r\n",
    "segments = [slogan_tkn] * SEQ_LEN\r\n",
    "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\r\n",
    "\r\n",
    "input_ids += [slogan_tkn]\r\n",
    "\r\n",
    "\r\n",
    "epochs = [3, 6, 10]\r\n",
    "for epoch in epochs:\r\n",
    "  model.load_state_dict(torch.load(MODEL_PATH+ '/' + f'processed_slogan_{epoch}epoch_model.pth'))\r\n",
    "  model.eval()\r\n",
    "\r\n",
    "  # 최개길이 20의 20개의 슬로건 샘플\r\n",
    "  # 확률분포를 조금 뾰족하게 하여 확률값이 높은 토큰이 살짝 더 잘나오도록 (temperature=0.9)\r\n",
    "  # top_k 샘플링을 적용하여 확률값이 낮은 토큰들은 후보 단어에서 배제 (top_k=5)\r\n",
    "  generated = sample_sequence(model, length=30, context=input_ids, segments_tokens=segments, temperature=0.7, top_k=40, top_p=0.8, num_samples=20)\r\n",
    "\r\n",
    "  print('\\n\\n--- Generated Slogans ---\\n')\r\n",
    "\r\n",
    "  for g in generated:\r\n",
    "    slogan = tokenizer.decode(g.squeeze().tolist())\r\n",
    "    slogan = slogan.split('</s>')[0].split('<unused1>')[1]\r\n",
    "    print(slogan)   "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30/30 [00:28<00:00,  1.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 한끼의 힘을 위해\n",
      " 당신이 당신이 필요한 순간\n",
      " 또 없던 가장 가까운 편안하게\n",
      " 당신이 다른 세상\n",
      " 한 번, 이제 우리 여행\n",
      " 우리에게 더 좋은 여행한 모든 것\n",
      " 건강한 여름의 시작\n",
      " 지금도 함께하면 진짜 한끼의 시작\n",
      " 우리만의 주일도 행복\n",
      " 더 가까운 새로운 한 번째 당신이 봅니다\n",
      " 한겨울 한끼, 다 없다\n",
      " 한 번째 새로운 순간\n",
      " 한끼의 시작\n",
      " 또 필요한 새로운 세상을 위해\n",
      " 당신이 다른 커피\n",
      " 한 잔, 이제 함께하면 더 큰 대한민국\n",
      " 대한민국, 한끼, 새로운 선택\n",
      " 이제 좋은 모든 것을 넘어 힘\n",
      " 지금도 행복\n",
      " 좋은 순간\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30/30 [00:28<00:00,  1.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 내 스타일리이다\n",
      " 여름은 커마로 짜릿하게\n",
      " 세상에서 가장 가장 가장 가장 더 부드럽게\n",
      " 여름은 더 더 좋은 것\n",
      " 더 좋아하는 커피를 만나다\n",
      " 오늘은 즐거움\n",
      " 커피를 만나다\n",
      " 이걸거보다 더 많은걸레는 한 알 수 없는 더 맛있는지\n",
      " 내 커피를 넘어, 이 손 안에다\n",
      " 이 여름은 한 번도 더 가까워집니다\n",
      " 커피의 경계를 만나다\n",
      " 오늘은 즐거움\n",
      " 오늘은 커피를 넘어\n",
      " 가장 좋아하는 카페, 이걸렘, 더 빛나는 순간\n",
      " 오늘도 늘매일매일 짜릿하게\n",
      " 여름은 쉬워라\n",
      " 스타마피는 더 좋은 순간\n",
      " 더 더 좋아하는 커피를 위한 새로운 방법\n",
      " 커피의 상식을 채우다\n",
      " 여름, 스타일이 되다\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30/30 [00:28<00:00,  1.04it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 커피의 창조남\n",
      " 커피의 창조\n",
      " 커피에 대한 순간, 필틱링의 시작\n",
      " 커피를 입는 순간, 모든 순간\n",
      " 우리의 커피를 위해 모든 순간, 이 곳을 담아\n",
      " 커피에 대한 모든 순간\n",
      " 커피에 커피 한잔의 창조 담아\n",
      " 우리의 커피를 위한 가장 빛나는 순간\n",
      " 커피의 창조봄, 모든 모든 것을 새롭게\n",
      " 커피의 창조다\n",
      " 모든 차원의 커피를 위한 커피\n",
      " 커피를 입는 순간, 이기는 향합니다\n",
      " 더 맛있는 순간\n",
      " 이 곳을 즐기는 향\n",
      " 커피에 대한 곳에\n",
      " 커피의 창조 않는다\n",
      " 커피의 창조\n",
      " 커피의 창조\n",
      " 커피의 모든 순간\n",
      " 커피의 창조 채운다\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for epoch in epochs:\r\n",
    "  model.load_state_dict(torch.load(MODEL_PATH+ '/' + f'processed_slogan_{epoch}epoch_model.pth'))\r\n",
    "  model.eval()\r\n",
    "\r\n",
    "  # 최개길이 20의 20개의 슬로건 샘플\r\n",
    "  # 확률분포를 조금 뾰족하게 하여 확률값이 높은 토큰이 살짝 더 잘나오도록 (temperature=0.9)\r\n",
    "  # top_k 샘플링과 top_p 샘플링을 동시에 적용하여 확률값이 낮은 토큰들은 후보 단어에서 배제 (top_k=50, top_p=0.95)\r\n",
    "  generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, temperature=0.9, top_k=20, top_p=0.95, num_samples=20)\r\n",
    "\r\n",
    "  print('\\n\\n--- Generated Slogans ---\\n')\r\n",
    "\r\n",
    "  for g in generated:\r\n",
    "    slogan = tokenizer.decode(g.squeeze().tolist())\r\n",
    "    slogan = slogan.split('</s>')[0].split('<unused1>')[1]\r\n",
    "    print(slogan)   \r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.34it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 모든 사람은은 당신의 쉐드\n",
      " 나만의 세상\n",
      " 올 한잔, 내 폰은 좋은 맛\n",
      " 우리에 더 없던 행복합니다\n",
      " 오늘은 쉐밍\n",
      " 좋은 순간\n",
      " 우리 가족부터 대한민국, 세쾌한일?\n",
      " 새로운 겨울으로 더 가까운 순간\n",
      " 대한민국 우리 나만의 새로운 길을 위해\n",
      " 모든 사람은 넌 더 건강한 맛있는 기술\n",
      " 당신이 필요한 당신이 챗\n",
      " 한 번째 is me\n",
      " 내 여름은 더 바르게 위해\n",
      " 올일은 지금부터 좋은 순간\n",
      " 올 세상, 당신이 다른 여행\n",
      " 우리만의 세상이 있습니다. 우리 가족입니다\n",
      " 이제, 당신이 다른 집합니다\n",
      " 이제 올 여름은 당신과 함께\n",
      " SUE BLURPY BHE\n",
      " 우리만의 당신이 더 나은 것을 위해\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 이 커피의 새로운 시작\n",
      " 스타마만의 일상을 바꾸다\n",
      " 당신의 순간, 새로운 길을 찾아\n",
      " 세상에 없던 쇼핑, 카페, 시작\n",
      " 여름이 필요한 것을 한 조각이 됩니다\n",
      " 여름이 되다, 맥주니까\n",
      " 당신의 프리엄 시작은 더 큰 순간이 있다\n",
      " 한잔이 온만의 레시피\n",
      " 모든 순간, 여름\n",
      " 여름은 더 좋아하는커피니까\n",
      " 커피를 한잔를 만나다\n",
      " 여름보다 쿨하게 가득의 새로운\n",
      " 커피를 한잔의 선택\n",
      " 오늘은 더 빛나는 모든 순간\n",
      " 세상에 없던 카페\n",
      " 오늘은 커피의 새로운 선택\n",
      " 올 겨울에 더 완벽한 청정함을 만나다\n",
      " 커피이 한잔이 바뀐다\n",
      " 오늘은 시작은 즐거울수록 행복은 아니다\n",
      " 상큼이 다른 세상\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.34it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 커피에 다르게 하다\n",
      " 커피의 여유를 채운다\n",
      " 커피의 창조\n",
      " 더 완벽한 프리미엄 커피브\n",
      " 커피의 여유를 채운다\n",
      " 우리의 행복이 커피는 놀라운 꽃\n",
      " 커피에 모든 것을 다르게합니다\n",
      " 커피의 모든 순간\n",
      " 커피, 한 잔의 모든 것\n",
      " 커피에 대한 기대를 위한 모든 순간\n",
      " 커피에 대한 다르게 더 더 예쁘네\n",
      " 다르게 즐기는 향\n",
      " 당신의 차원의 커피를 새롭게\n",
      " 커피를 부른다\n",
      " 커피를 넘어 모든 경계를 위해 더 큰 옷을 위하여\n",
      " 내 스타벅스의 커피를 힐링를 만나다\n",
      " 내 노력의만의 차원의 커피를 부른다\n",
      " 커피를 입는 순간도 화릿하게\n",
      " 커피의 모든 도전이 새롭게 채운다\n",
      " 커피에 필링의 모든 순간\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "context = \"어쌔씬크리드 발할라, 액션 게임\"\r\n",
    "\r\n",
    "context_tkn = tokenizer.additional_special_tokens_ids[0]\r\n",
    "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\r\n",
    "\r\n",
    "input_ids = [context_tkn] + tokenizer.encode(context)\r\n",
    "\r\n",
    "segments = [slogan_tkn] * SEQ_LEN\r\n",
    "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\r\n",
    "\r\n",
    "input_ids += [slogan_tkn]\r\n",
    "\r\n",
    "\r\n",
    "epochs = [6, 10]\r\n",
    "for epoch in epochs:\r\n",
    "  model.load_state_dict(torch.load(MODEL_PATH+ '/' + f'processed_slogan_{epoch}epoch_model.pth'))\r\n",
    "  model.eval()\r\n",
    "\r\n",
    "  # 최개길이 20의 20개의 슬로건 샘플\r\n",
    "  # 확률분포를 조금 뾰족하게 하여 확률값이 높은 토큰이 살짝 더 잘나오도록 (temperature=0.9)\r\n",
    "  # top_k 샘플링을 적용하여 확률값이 낮은 토큰들은 후보 단어에서 배제 (top_k=5)\r\n",
    "  generated = sample_sequence(model, length=30, context=input_ids, segments_tokens=segments, temperature=0.8, top_k=40, top_p=0.85, num_samples=20)\r\n",
    "\r\n",
    "  print('\\n\\n--- Generated Slogans ---\\n')\r\n",
    "\r\n",
    "  for g in generated:\r\n",
    "    slogan = tokenizer.decode(g.squeeze().tolist())\r\n",
    "    slogan = slogan.split('</s>')[0].split('<unused1>')[1]\r\n",
    "    print(slogan)   "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30/30 [00:37<00:00,  1.25s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 모든 플레이이 필요한 것\n",
      " 모든 것형가 다른 모든 순간\n",
      " 비짝간의 고상의 초미\n",
      " 오늘은 아빠은 아니다\n",
      " RPG, SORT, WAUL를 가져라\n",
      " 모바일와 감온 전감\n",
      " 새로운 신세계\n",
      " 새로운 플레이\n",
      " 그 이상\n",
      " 당신이 위해\n",
      " 처음 만나는 즐거움\n",
      " 너를 넘어, 전 플레이의 선택\n",
      " 너만의 스타일로 만나는 모든 것\n",
      " 더 만나는 즐거움\n",
      " 모바일만의 야리의 선택\n",
      " 이것은 초미먼지, 모든 순간\n",
      " 모바일 골프를 플레이하라\n",
      " SIGHE THEAYE\n",
      " 너를 넘어, 이제, 바로 플레이\n",
      " 그 이상가 된다\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 30/30 [00:36<00:00,  1.23s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " 이제, 모든 곳이 만나는 소녀\n",
      " 카바일로 이 위대한 가문전쟁\n",
      " 진짜 스타일에 자유를 즐겨라\n",
      " 다시, 모든것이다\n",
      " 진짜 이제 모든 순간, 항상 연맹\n",
      " 이 세상 모든 순간, 부딪쳐\n",
      " 진짜 세상, 이 땅을 위한 스타일로\n",
      " 레전볼루션의 시작과 판스, 시작\n",
      " 내 방식대로 즐겨라\n",
      " 이제, 모든 순간\n",
      " RePadelful your Chour space\n",
      " 이제 내 손에서 시작\n",
      " 비거도 스타일은 영웅, 시작은 역시\n",
      " 지금, 스타일은 나만의 리그을 위해\n",
      " 내 안의 도전이 플레이이 함께라면 스타릴 것\n",
      " 이제 지금도 게임을 쥐는 스타일은 스타일은 재밌다\n",
      " 내 방식대로 즐기는 진짜 가을\n",
      " 내 몸의 팀은 이긴다\n",
      " 모든 순간이 감던 타벨함을 즐겨봐\n",
      " 이것은 내 몸을 이 번째 식\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)"
  },
  "interpreter": {
   "hash": "293ef13038b1144d4811de228cdfb91e615f2f48e1a0c87d3a386cf88ee0761d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}