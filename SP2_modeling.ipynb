{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# 변수 선언\r\n",
    "\r\n",
    "MODEL_NAME = \"skt/kogpt2-base-v2\"\r\n",
    "DATA_IN_PATH = \"./datasets/\"\r\n",
    "DATA_OUT_PATH = \"./models/\"\r\n",
    "TRAIN_DATA_FILE = \"processed_slogan_final.csv\"\r\n",
    "TRAIN_DATA_NAME = \"processed_slogan_final\"\r\n",
    "PLOT_OUT_PATH = \"./plots/\"\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from transformers import PreTrainedTokenizerFast\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "\r\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\r\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "SPECIAL_TOKENS_DICT  = {\r\n",
    "    'eos_token':'</s>',\r\n",
    "    'pad_token':'<pad>',\r\n",
    "    'additional_special_tokens':['<unused0>', '<unused1>'],  # 설명, 슬로건 컬럼을 나누기 위해 kogpt2의 미사용 토큰을 특수토큰으로 추가 (미사용 토큰을 사용하여 모델 임베딩 크기 조절이 필요없음)\r\n",
    "}\r\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\r\n",
    "\r\n",
    "print(tokenizer.special_tokens_map)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eos_token': '</s>', 'pad_token': '<pad>', 'additional_special_tokens': \"['<unused0>', '<unused1>']\"}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# # 한정된 GPU 메모리를 최대로 활용하기 위한, 데이터 길이 확인\r\n",
    "# import csv\r\n",
    "# import pandas as pd\r\n",
    "# import matplotlib.pyplot as plt\r\n",
    "# plt.style.use('seaborn')\r\n",
    "\r\n",
    "# filename =  DATA_IN_PATH + TRAIN_DATA_FILE\r\n",
    "\r\n",
    "# context_tkn = tokenizer.additional_special_tokens_ids[0]  # 토크나이저의 additional_special_tokens_ids[0] : <unused0>\r\n",
    "# slogan_tkn = tokenizer.additional_special_tokens_ids[1] # 토크나이저의 additional_special_tokens_ids[1] : <unused1>\r\n",
    "# pad_tkn = tokenizer.pad_token_id  #'<pad>'\r\n",
    "# eos_tkn = tokenizer.eos_token_id  # </s>\r\n",
    "\r\n",
    "# with open(filename, 'r', encoding=\"euc-kr\") as csvfile:  # 수집된 슬로건 CSV파일 불러오기\r\n",
    "#     reader = csv.reader(csvfile)\r\n",
    "    \r\n",
    "#     # row별 길이를 정보를 담을 빈리스트 생성\r\n",
    "#     context_list = []\r\n",
    "#     slogan_list = []\r\n",
    "#     seq_len_list = []\r\n",
    "    \r\n",
    "#     for row in reader:\r\n",
    "#         context = [context_tkn] + tokenizer.encode(row[0])\r\n",
    "#         context_list.append(len(context))\r\n",
    "        \r\n",
    "#         slogan = [slogan_tkn] + tokenizer.encode(row[1]) + [eos_tkn]\r\n",
    "#         slogan_list.append(len(slogan))\r\n",
    "\r\n",
    "#         seq_len = context + slogan\r\n",
    "#         seq_len_list.append(len(seq_len))\r\n",
    "\r\n",
    "# # 리스트로 데이터프레임 생성\r\n",
    "# scatter_df = pd.DataFrame(context_list, columns=[\"context\"])\r\n",
    "# scatter_df['slogan'] = slogan_list\r\n",
    "# scatter_df['add_len'] = seq_len_list\r\n",
    "\r\n",
    "# # seaborn box plot을 활용하여 시각화\r\n",
    "# red_square = dict(markerfacecolor='r', markeredgecolor='r', marker='.')\r\n",
    "\r\n",
    "# scatter_df[['context', 'slogan', 'add_len']].plot(kind='box', vert=False, flierprops=red_square, figsize=(16,2));\r\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import csv\r\n",
    "import torch\r\n",
    "from torch.utils.data import Dataset\r\n",
    "\r\n",
    "# https://wikidocs.net/57165\r\n",
    "# 학습용 데이터 로더\r\n",
    "\r\n",
    "class SloganDataset(Dataset):\r\n",
    "  # 데이터셋의 전처리를 해주는 부분\r\n",
    "  def __init__(self, filename, tokenizer, seq_length=50): # seq_length : 위의 셀에서 확인한 길이 정보를 토대로 50 지정\r\n",
    "    \r\n",
    "    context_tkn = tokenizer.additional_special_tokens_ids[0]  # <unused0>\r\n",
    "    slogan_tkn = tokenizer.additional_special_tokens_ids[1] # <unused1>\r\n",
    "    pad_tkn = tokenizer.pad_token_id  #'<pad>'\r\n",
    "    eos_tkn = tokenizer.eos_token_id  # </s>\r\n",
    "\r\n",
    "    self.examples = []  # example 빈리스트 생성\r\n",
    "    with open(filename, 'r', encoding=\"UTF-8\") as csvfile:\r\n",
    "      reader = csv.reader(csvfile)\r\n",
    "      \r\n",
    "      for row in reader:\r\n",
    "        # 컨텍스트 및 슬로건 세그먼트 구축\r\n",
    "        context = [context_tkn] + tokenizer.encode(row[0])\r\n",
    "        slogan = [slogan_tkn] + tokenizer.encode(row[1]) + [eos_tkn]\r\n",
    "        \r\n",
    "        # 두 부분을 함께 연결\r\n",
    "        tokens = context + slogan + [pad_tkn] * ( seq_length - len(context) - len(slogan) ) # 50 길이만큼 </pad>토큰 채움\r\n",
    "\r\n",
    "        # 해당 세그먼트로 각 토큰에 주석달기\r\n",
    "        segments = [context_tkn] * len(context) + [slogan_tkn] * ( seq_length - len(context) )  \r\n",
    "\r\n",
    "        # 레이블을 -100으로 설정하여 컨텍스트, 패딩 및 <unused1> 토큰을 무시\r\n",
    "        labels = [-100] * (len(context)+1) + slogan[1:] + [-100] * ( seq_length - len(context) - len(slogan) )\r\n",
    "\r\n",
    "        # 데이터셋에 전처리된 예제 추가\r\n",
    "        self.examples.append((tokens, segments, labels)) #[토큰, 세그먼트, 레이블]\r\n",
    "  # 데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\r\n",
    "  # len(dataset)을 했을 때 데이터셋의 크기를 리턴할 len\r\n",
    "  def __len__(self):\r\n",
    "    return len(self.examples)\r\n",
    "\r\n",
    "  # 데이터셋에서 특정 1개의 샘플을 가져오는 함수\r\n",
    "  # dataset[i]을 했을 때 i번째 샘플을 가져오도록 하는 인덱싱을 위한 get_item\r\n",
    "  def __getitem__(self, item):\r\n",
    "    return torch.tensor(self.examples[item])\r\n",
    "\r\n",
    "\r\n",
    "# 데이터세트를 빌드하고 검증을 위해 첫 번째 배치의 차원을 표시\r\n",
    "slogan_dataset = SloganDataset(DATA_IN_PATH + TRAIN_DATA_FILE, tokenizer)\r\n",
    "print(next(iter(slogan_dataset)).size())"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/processed_slogan_final.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-170a1c4075f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m# 데이터세트를 빌드하고 검증을 위해 첫 번째 배치의 차원을 표시\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mslogan_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSloganDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_IN_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mTRAIN_DATA_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslogan_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-170a1c4075f6>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, tokenizer, seq_length)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# example 빈리스트 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"UTF-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m       \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/processed_slogan_final.csv'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math, random\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\r\n",
    "\r\n",
    "# 훈련 및 검증 데이터 분할을 위한 인덱스 생성\r\n",
    "indices = list(range(len(slogan_dataset)))\r\n",
    "\r\n",
    "random.seed(100) #난수 생성\r\n",
    "random.shuffle(indices)\r\n",
    "\r\n",
    "split = math.floor(0.1 * len(slogan_dataset))\r\n",
    "train_indices, val_indices = indices[split:], indices[:split]\r\n",
    "\r\n",
    "# PyTorch 데이터 로더를 빌드\r\n",
    "train_sampler = SubsetRandomSampler(train_indices)\r\n",
    "val_sampler = SubsetRandomSampler(val_indices)\r\n",
    "\r\n",
    "train_loader = DataLoader(slogan_dataset, batch_size=32, sampler=train_sampler)\r\n",
    "val_loader = DataLoader(slogan_dataset, batch_size=64, sampler=val_sampler)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "import os\r\n",
    "\r\n",
    "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\r\n",
    "\r\n",
    "  if not os.path.exists(DATA_OUT_PATH):\r\n",
    "    os.makedirs(DATA_OUT_PATH)\r\n",
    "    print('--- Directory creation completed successfully ---')\r\n",
    "  else:\r\n",
    "    print('--- Directory already exists ---')\r\n",
    "  \r\n",
    "  # epoch당 average training loss를 track\r\n",
    "  # epoch당 average validation loss를 track\r\n",
    "  avg_train_losses = []\r\n",
    "  avg_valid_losses = []\r\n",
    "  \r\n",
    "  for i in range(epochs):\r\n",
    "\r\n",
    "    print(f'\\n--- Starting epoch #{i+1} ---')\r\n",
    "\r\n",
    "    model.train()\r\n",
    "    \r\n",
    "    # 한 epoch 동안 배치 손실과 배치 크기를 추적을 위한 리스트 생성\r\n",
    "    losses = []\r\n",
    "    nums = []\r\n",
    "\r\n",
    "    for xb in tqdm(train_dl, desc=\"Training\"):\r\n",
    "      # 배치를 훈련 장치로 이동\r\n",
    "      inputs = xb.to(device)\r\n",
    "\r\n",
    "      # 토큰 ID, 세그먼트 ID 및 정답(레이블)을 사용하여 모델을 호출\r\n",
    "      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\r\n",
    "      \r\n",
    "      # 목록에 손실 및 배치 크기를 추가\r\n",
    "      loss = outputs[0]\r\n",
    "      losses.append(loss.item())\r\n",
    "      nums.append(len(xb))\r\n",
    "\r\n",
    "      loss.backward()\r\n",
    "      # xm.optimizer_step(optimizer, barrier=True)  # 이부분이 TPU 쓸때 필요한 코드!!\r\n",
    "      optimizer.step()\r\n",
    "      model.zero_grad()\r\n",
    "\r\n",
    "    # 한 epoch 동안의 평균 비용을 계산\r\n",
    "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\r\n",
    "    avg_train_losses.append(train_cost)\r\n",
    "\r\n",
    "    # 이제 유효성 검사를 위해 동일한 작업을 수행\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    with torch.no_grad():\r\n",
    "      losses = []\r\n",
    "      nums = []\r\n",
    "\r\n",
    "      for xb in tqdm(val_dl, desc=\"Validation\"):\r\n",
    "        inputs = xb.to(device)\r\n",
    "        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\r\n",
    "        losses.append(outputs[0].item())\r\n",
    "        nums.append(len(xb))\r\n",
    "\r\n",
    "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\r\n",
    "    avg_valid_losses.append(val_cost)\r\n",
    "    \r\n",
    "    print(f'\\n--- Epoch #{i+1} finished --- Training cost: {train_cost} / Validation cost: {val_cost}')\r\n",
    "  \r\n",
    "    if (i + 1) % 1 == 0 :\r\n",
    "      torch.save(model.state_dict(), DATA_OUT_PATH + TRAIN_DATA_NAME + '_' + f'{i+1}epoch' + '_' + 'model.pth')\r\n",
    "      print(f'\\n--- Epoch #{i+1} Saving complete ! ---')\r\n",
    "      \r\n",
    "    torch.cuda.empty_cache()\r\n",
    "      \r\n",
    "  return avg_train_losses, avg_valid_losses \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 이부분이 TPU 쓸때 필요한 코드!! fit 함수확인\r\n",
    "# import torch_xla\r\n",
    "# import torch_xla.core.xla_model as xm\r\n",
    "\r\n",
    "# # Creates AlexNet for 10 classes\r\n",
    "# net = torchvision.models.alexnet(num_classes=10)\r\n",
    "\r\n",
    "# # Acquires the default Cloud TPU core and moves the model to it\r\n",
    "# device = xm.xla_device()\r\n",
    "# net = net.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import AdamW\r\n",
    "\r\n",
    "# Move the model to the GPU:\r\n",
    "device = torch.device('cuda')\r\n",
    "model.to(device)\r\n",
    "\r\n",
    "# Fine-tune GPT2 for 5 epochs: \r\n",
    "optimizer = AdamW(model.parameters()) # 트랜스포머의 AdamW\r\n",
    "train_loss, valid_loss = fit(model, optimizer, train_loader, val_loader, epochs=10, device=device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 0/290 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Directory already exists ---\n",
      "\n",
      "--- Starting epoch #1 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 290/290 [01:24<00:00,  3.42it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  5.51it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #1 finished --- Training cost: 7.123012450732287 / Validation cost: 6.4783869582084455\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 1/290 [00:00<00:36,  7.93it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #1 Saving complete ! ---\n",
      "\n",
      "--- Starting epoch #2 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:02<00:00,  5.72it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #2 finished --- Training cost: 6.0926285541206795 / Validation cost: 6.15883055511786\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 1/290 [00:00<00:36,  8.02it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #2 Saving complete ! ---\n",
      "\n",
      "--- Starting epoch #3 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  5.62it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #3 finished --- Training cost: 5.632176023647093 / Validation cost: 6.001226851497609\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 1/290 [00:00<00:37,  7.63it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #3 Saving complete ! ---\n",
      "\n",
      "--- Starting epoch #4 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 290/290 [01:23<00:00,  3.48it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  5.54it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #4 finished --- Training cost: 5.258526659855618 / Validation cost: 5.917583089759908\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 1/290 [00:00<00:36,  7.87it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #4 Saving complete ! ---\n",
      "\n",
      "--- Starting epoch #5 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 290/290 [01:26<00:00,  3.37it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  5.59it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #5 finished --- Training cost: 4.917307882510643 / Validation cost: 5.9214111889764105\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 1/290 [00:00<00:37,  7.81it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #5 Saving complete ! ---\n",
      "\n",
      "--- Starting epoch #6 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 290/290 [01:23<00:00,  3.46it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:02<00:00,  6.00it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #6 finished --- Training cost: 4.6015630653308905 / Validation cost: 5.903548597478542\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 1/290 [00:00<00:34,  8.29it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #6 Saving complete ! ---\n",
      "\n",
      "--- Starting epoch #7 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 290/290 [01:23<00:00,  3.47it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  5.55it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #7 finished --- Training cost: 4.318176719820288 / Validation cost: 5.905958679373456\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 1/290 [00:00<00:36,  7.87it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #7 Saving complete ! ---\n",
      "\n",
      "--- Starting epoch #8 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 290/290 [01:22<00:00,  3.51it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  5.50it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #8 finished --- Training cost: 4.039858158055595 / Validation cost: 6.0391718495575635\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 1/290 [00:00<00:36,  7.93it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #8 Saving complete ! ---\n",
      "\n",
      "--- Starting epoch #9 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 290/290 [01:23<00:00,  3.46it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:03<00:00,  5.51it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #9 finished --- Training cost: 3.7935600649142605 / Validation cost: 6.191389684890288\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training:   0%|          | 1/290 [00:00<00:36,  7.93it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #9 Saving complete ! ---\n",
      "\n",
      "--- Starting epoch #10 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training: 100%|██████████| 290/290 [01:23<00:00,  3.49it/s]\n",
      "Validation: 100%|██████████| 17/17 [00:02<00:00,  5.88it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Epoch #10 finished --- Training cost: 3.628651342568955 / Validation cost: 6.267690752175853\n",
      "\n",
      "--- Epoch #10 Saving complete ! ---\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "\r\n",
    "# 훈련이 진행되는 과정에 따라 loss를 시각화\r\n",
    "fig = plt.figure(figsize=(10,8))\r\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\r\n",
    "plt.plot(range(1, len(valid_loss) + 1), valid_loss, label='Validation Loss')\r\n",
    "\r\n",
    "# validation loss의 최저값 지점을 찾기\r\n",
    "minposs = valid_loss.index(min(valid_loss)) + 1\r\n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\r\n",
    "\r\n",
    "plt.xlabel('epochs')\r\n",
    "plt.ylabel('loss')\r\n",
    "plt.ylim(0, 8) # 일정한 scale\r\n",
    "plt.xlim(0, len(train_loss)+1) # 일정한 scale\r\n",
    "plt.grid(True)\r\n",
    "plt.legend()\r\n",
    "plt.tight_layout()\r\n",
    "plt.show()\r\n",
    "\r\n",
    "if not os.path.exists(PLOT_OUT_PATH):\r\n",
    "    os.makedirs(PLOT_OUT_PATH)\r\n",
    "    print('--- Directory creation completed successfully ---')\r\n",
    "    \r\n",
    "else:\r\n",
    "    print('--- Directory already exists ---')\r\n",
    "\r\n",
    "fig.savefig( PLOT_OUT_PATH + TRAIN_DATA_NAME + '_loss_plot.png', bbox_inches = 'tight')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)"
  },
  "interpreter": {
   "hash": "293ef13038b1144d4811de228cdfb91e615f2f48e1a0c87d3a386cf88ee0761d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}