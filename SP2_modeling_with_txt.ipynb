{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "import pandas as pd\r\n",
    "import re\r\n",
    "\r\n",
    "def cleaning(s):\r\n",
    "  s = str(s)\r\n",
    "  s = re.sub(', ', ',', s)\r\n",
    "  s = re.sub(',', ' ', s)\r\n",
    "  return s\r\n",
    "\r\n",
    "df = pd.read_csv(\"./datasets/final.csv\", encoding=\"utf-8\")\r\n",
    "df = df.dropna()\r\n",
    "text_data = open('./datasets/slogans.txt', 'w', encoding=\"utf-8\")\r\n",
    "for idx, item in df.iterrows():\r\n",
    "  slogans = cleaning(item['company']) + ', ' + item['slogan']+ '\\n'\r\n",
    "  text_data.write(slogans)\r\n",
    "text_data.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\r\n",
    "from transformers import GPT2LMHeadModel\r\n",
    "from transformers import Trainer, TrainingArguments\r\n",
    "from transformers import PreTrainedTokenizerFast\r\n",
    "\r\n",
    "def load_dataset(file_path, tokenizer, block_size = 128):\r\n",
    "    dataset = TextDataset(\r\n",
    "        tokenizer = tokenizer,\r\n",
    "        file_path = file_path,\r\n",
    "        block_size = block_size,\r\n",
    "    )\r\n",
    "    return dataset\r\n",
    "\r\n",
    "\r\n",
    "def load_data_collator(tokenizer, mlm = False):\r\n",
    "    data_collator = DataCollatorForLanguageModeling(\r\n",
    "        tokenizer=tokenizer, \r\n",
    "        mlm=mlm,\r\n",
    "    )\r\n",
    "    return data_collator\r\n",
    "\r\n",
    "def train(train_file_path,model_name,\r\n",
    "          output_dir,\r\n",
    "          overwrite_output_dir,\r\n",
    "          per_device_train_batch_size,\r\n",
    "          num_train_epochs,\r\n",
    "          save_steps):\r\n",
    "  tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\r\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\r\n",
    "  data_collator = load_data_collator(tokenizer)\r\n",
    "\r\n",
    "  tokenizer.save_pretrained(output_dir, legacy_format=False)\r\n",
    "   \r\n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\r\n",
    "\r\n",
    "  model.save_pretrained(output_dir)\r\n",
    "\r\n",
    "  training_args = TrainingArguments(\r\n",
    "          output_dir=output_dir,\r\n",
    "          overwrite_output_dir=overwrite_output_dir,\r\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\r\n",
    "          num_train_epochs=num_train_epochs,\r\n",
    "      )\r\n",
    "\r\n",
    "  trainer = Trainer(\r\n",
    "          model=model,\r\n",
    "          args=training_args,\r\n",
    "          data_collator=data_collator,\r\n",
    "          train_dataset=train_dataset,\r\n",
    "  )\r\n",
    "      \r\n",
    "  trainer.train()\r\n",
    "  trainer.save_model()\r\n",
    "\r\n",
    "train_file_path = './datasets/slogans.txt'\r\n",
    "model_name = 'skt/kogpt2-base-v2'\r\n",
    "output_dir = './models2'\r\n",
    "overwrite_output_dir = False\r\n",
    "per_device_train_batch_size = 8\r\n",
    "num_train_epochs = 5.0\r\n",
    "save_steps = 500\r\n",
    "\r\n",
    "train(\r\n",
    "    train_file_path=train_file_path,\r\n",
    "    model_name=model_name,\r\n",
    "    output_dir=output_dir,\r\n",
    "    overwrite_output_dir=overwrite_output_dir,\r\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\r\n",
    "    num_train_epochs=num_train_epochs,\r\n",
    "    save_steps=save_steps\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 49%|████▉     | 500/1020 [01:46<01:52,  4.61it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 2.4771, 'learning_rate': 2.5490196078431373e-05, 'epoch': 2.45}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 98%|█████████▊| 1000/1020 [03:36<00:04,  4.92it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 1.6098, 'learning_rate': 9.80392156862745e-07, 'epoch': 4.9}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1020/1020 [03:42<00:00,  4.59it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train_runtime': 222.1263, 'train_samples_per_second': 4.592, 'epoch': 5.0}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\r\n",
    "\r\n",
    "def load_model(model_path):\r\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\r\n",
    "    return model\r\n",
    "\r\n",
    "\r\n",
    "def load_tokenizer(tokenizer_path):\r\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\r\n",
    "    return tokenizer\r\n",
    "\r\n",
    "\r\n",
    "def generate_text(sequence, max_length):\r\n",
    "    model_path = \"./models2\"\r\n",
    "    model = load_model(model_path)\r\n",
    "    tokenizer = load_tokenizer(model_path)\r\n",
    "    ids = tokenizer.encode(f'{sequence},', return_tensors='pt')\r\n",
    "    final_outputs = model.generate(\r\n",
    "        ids,\r\n",
    "        do_sample=True,\r\n",
    "        max_length=max_length,\r\n",
    "        pad_token_id=model.config.pad_token_id,\r\n",
    "        top_k=50,\r\n",
    "        top_p=0.95,\r\n",
    "    )\r\n",
    "    return(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\r\n",
    "\r\n",
    "# sequence = input()\r\n",
    "# max_len = int(input())\r\n",
    "\r\n",
    "company = \"닥스\" #(최대 10글자)\r\n",
    "context = \"셔츠 제조업\" #(최대 글자 지정예정)\r\n",
    "\r\n",
    "input = company + ', ' + context \r\n",
    "\r\n",
    "sequence = input\r\n",
    "max_len = 50\r\n",
    "\r\n",
    "print('input :' + sequence)\r\n",
    "\r\n",
    "\r\n",
    "for i in range(10):\r\n",
    "    print('=' * 50)\r\n",
    "    g = generate_text(sequence, max_len)\r\n",
    "    g = g.split('<eos>')[0]\r\n",
    "    g = g.split(',')[2:]\r\n",
    "    g = ', '.join(g)\r\n",
    "    print(g)\r\n",
    "    \r\n",
    "print('=' * 50)\r\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input :닥스, 셔츠 제조업\n",
      "==================================================\n",
      " 이뻐지자\n",
      "==================================================\n",
      " 봄을 즐겨요\n",
      "==================================================\n",
      " 바람막이로써 바람막이를\n",
      "==================================================\n",
      " 스포츠 의류업비거리,  상식을 뒤집다\n",
      "==================================================\n",
      " 누구나 지금 웃고 있습니다.\n",
      "지금까지 누우자,  전시,  컨벤션 및 행사 대행업Love you are\n",
      "==================================================\n",
      " 다운자신감이 느껴진다\n",
      "==================================================\n",
      " 당신에게 선물하세요\n",
      "==================================================\n",
      " 셔츠 제조업요즘에는 셔츠로 더 사나워\n",
      "==================================================\n",
      " 캐주얼한 의상은 당신의 마음을 움직이다\n",
      "==================================================\n",
      " 운동 및 경기용품 소매업스타일은 스타일대로\n",
      "==================================================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    " # 슬로건, 설명 분리를 하지않을시 특정 input 에서 문제 발생'\r\n",
    " \r\n",
    " '''\r\n",
    "input :어쌔씬크리드 발할라, 액션게임\r\n",
    "==================================================\r\n",
    " 액션을 넘어 액션까지\r\n",
    "==================================================\r\n",
    " 액션 어쌔씬하게 살아나다\r\n",
    "==================================================\r\n",
    " 액션게임의 새로운 장르\r\n",
    "==================================================\r\n",
    " 이제 액션할 때\r\n",
    "==================================================\r\n",
    " 그래서 액션이다\r\n",
    "==================================================\r\n",
    " 액션을 넘나들다\r\n",
    "==================================================\r\n",
    " 액션에 빠지다\r\n",
    "==================================================\r\n",
    " 그 어디에서도 볼 수 없었던 리얼로드,  액션의 신세계\r\n",
    "==================================================\r\n",
    " 액션이제 액션도 액션이니까\r\n",
    "==================================================\r\n",
    " 액션이것이 액션의 시작\r\n",
    "==================================================\r\n",
    "\r\n",
    "\r\n",
    " input :벼룩마켓, 구직 신문\r\n",
    "==================================================\r\n",
    " 구인구직 정보 플랫폼취업,  구직 정보 플랫폼취업,  구직 정보 플랫폼취업,  구직 정보 플랫폼취업,  구직 정보 플랫폼취업의 기준은\r\n",
    "==================================================\r\n",
    " 방송 프로그램 배너광고가 무슨 의미가 있나요?\r\n",
    "==================================================\r\n",
    " 온라인 정보매개 사이트올 추석에도 매장은 즐거운 기분\r\n",
    "==================================================\r\n",
    " 잡지,  인쇄업일취월장,  구인구직 정보 플랫폼\r\n",
    "==================================================\r\n",
    " 광고 대행업스펙은 스펙대로\r\n",
    "==================================================\r\n",
    " 데이터베이스 제공업선진국 온오프라인에서 구직 정보를 얻다\r\n",
    "==================================================\r\n",
    " 구인구직 정보 플랫폼취업이 쉬워진다\r\n",
    "==================================================\r\n",
    " 사진 정보 제공업모두의 취업 플랫폼\r\n",
    "==================================================\r\n",
    " 잡지 판매업나를 기다리는 많은 구직 정보\r\n",
    "==================================================\r\n",
    " 데이터베이스 및 온라인정보 제공업내가 원하는 구직 정보 플랫폼\r\n",
    "==================================================\r\n",
    "\r\n",
    "\r\n",
    " input :커피팩토리, 정말 맛있는 커피와 원두를 즐길 수 있는 공간\r\n",
    "==================================================\r\n",
    " 세계특허를 얻다\r\n",
    "==================================================\r\n",
    " 프리미엄 커피를 위한 프리미엄 커피\r\n",
    "==================================================\r\n",
    " 로스팅!\r\n",
    "==================================================\r\n",
    " 콜롬비아나에서 찾은 커피 한잔\r\n",
    "==================================================\r\n",
    " 이 곳은 카페\r\n",
    "==================================================\r\n",
    " 이것이 스페샬 오디너의 공식\r\n",
    "==================================================\r\n",
    " 원두커피의 향\r\n",
    "==================================================\r\n",
    " 오직 로스팅만으로 충분하다\r\n",
    "==================================================\r\n",
    " 세계 최저 수준의 카페인\r\n",
    "==================================================\r\n",
    " 원두토마스\r\n",
    "==================================================\r\n",
    "\r\n",
    "\r\n",
    "input :닥스, 셔츠 제조업\r\n",
    "==================================================\r\n",
    " 이뻐지자\r\n",
    "==================================================\r\n",
    " 봄을 즐겨요\r\n",
    "==================================================\r\n",
    " 바람막이로써 바람막이를\r\n",
    "==================================================\r\n",
    " 스포츠 의류업비거리,  상식을 뒤집다\r\n",
    "==================================================\r\n",
    " 누구나 지금 웃고 있습니다.\r\n",
    "지금까지 누우자,  전시,  컨벤션 및 행사 대행업Love you are\r\n",
    "==================================================\r\n",
    " 다운자신감이 느껴진다\r\n",
    "==================================================\r\n",
    " 당신에게 선물하세요\r\n",
    "==================================================\r\n",
    " 셔츠 제조업요즘에는 셔츠로 더 사나워\r\n",
    "==================================================\r\n",
    " 캐주얼한 의상은 당신의 마음을 움직이다\r\n",
    "==================================================\r\n",
    " 운동 및 경기용품 소매업스타일은 스타일대로\r\n",
    "==================================================\r\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)"
  },
  "interpreter": {
   "hash": "293ef13038b1144d4811de228cdfb91e615f2f48e1a0c87d3a386cf88ee0761d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}